import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import CNN
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from sklearn import model_selection
from sklearn import metrics
from sklearn.model_selection import train_test_split

#Read the data
data = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

#Convert to numpy arrays
X_train = data.iloc[:,1:].as_matrix()
y_train = data.iloc[:,0].as_matrix()
X_test = test.as_matrix()

#Normalization
X_train = X_train / 255.0
X_test = X_test / 255.0

#delete the pandas dataframes
del data, test

#Create network, criterion and optimizer
net = CNN.Net()
criterion = torch.nn.CrossEntropyLoss()
lr = 0.001
optimizer = optim.Adam(net.parameters(),lr=lr)
nr_epochs = 20


#Define train and test passages for learning and evaluation
def train(epoch):
	net.train()
	train_loss = 0
	acc_meas = [0, 0]
	b_id = 0
	total_sample, total_predictions = [], []
	
	# for each batch generated by DataLoader
	for batch_id, sample in enumerate(trainDataL):
		optimizer.zero_grad()  # Clear the gradients from our optimizer
		# inputs, targets = sample['X'], sample['Y']
		outputs = net(sample['X'])
		loss = criterion(outputs,sample['Y'])  # Calculate the loss between predictions and targets
		loss.backward()  # Get the gradient values
		optimizer.step()  # Update the weights of the optimizer with the gradients calculated
		
		train_loss += loss.item()  # add the loss of each batch to be able to calculate the overall loss
		_, predicted = torch.max(outputs.data, 1)
		
		total_sample += sample['Y'].tolist()
		total_predictions += predicted.reshape(-1).tolist()
		
		acc_meas[0] += metrics.accuracy_score(sample['Y'], predicted, normalize=False)
		acc_meas[1] += sample['Y'].size(0)
		b_id += 1
	
	#acc_tr[0].append(100. * acc_meas[0] / acc_meas[1])
	#loss_tr[0].append(train_loss / (b_id))
	print('TRAIN Epoch %d | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (
		epoch, train_loss / (b_id), 100. * acc_meas[0] / acc_meas[1], acc_meas[0], acc_meas[1]))
	if epoch == 29:
		print(metrics.classification_report(total_sample, total_predictions))


def test(epoch):
	net.eval()
	test_loss = 0
	acc_meas = [0, 0]
	b_id = 0
	
	# for each batch generated by DataLoader
	for batch_id, sample in enumerate(testDataL):
		outputs = net(sample['X'])
		loss = criterion(outputs, sample['Y'])  # Calculate the loss between predictions and targets
		
		test_loss += loss.item()  # add the loss of each batch to be able to calculate the overall loss
		_, predicted = torch.max(outputs.data, 1)
		acc_meas[1] += sample['Y'].size(0)
		acc_meas[0] += metrics.accuracy_score(sample['Y'], predicted, normalize=False)
		b_id += 1
	
	#acc_tr[1].append(100. * acc_meas[0] / acc_meas[1])
	#loss_tr[1].append(test_loss / (b_id))
	print('TEST  Epoch %d | Loss: %.3f | Acc: %.3f%% (%d/%d)' % (
		epoch, test_loss / (b_id), 100. * acc_meas[0] / acc_meas[1], acc_meas[0], acc_meas[1]))


#Split training data in different validation sets. This will make sure our network is trained over all training data
kf = model_selection.KFold(5)
batch_size = 64

#for train_index, test_index in kf.split(X_train):
#	X_t, X_ts = X_train[train_index,:], X_train[test_index,:]
#	y_t, y_ts = y_train[train_index], y_train[test_index]

X_t, X_ts, y_t, y_ts = train_test_split(X_train, y_train, test_size=0.1, random_state=42)

trainData = CNN.prepData(X_t, y_t)
trainDataL = DataLoader(trainData, batch_size=batch_size)

testData = CNN.prepData(X_ts, y_ts)
testDataL = DataLoader(testData, batch_size=batch_size)

epochs = nr_epochs
acc_tr, loss_tr = [[], []], [[], []]
for epoch in range(epochs):
	train(epoch)
	test(epoch)
	